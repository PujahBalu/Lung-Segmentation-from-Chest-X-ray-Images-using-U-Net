# -*- coding: utf-8 -*-
"""LungSegmentation_ChestXray_Images.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18RTIrUhkMmQ6v7TXO3WQQbsfXxe3Nhnb
"""

!pip install kaggle
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split

from google.colab import files
files.upload()  # Upload your kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download Dataset
!kaggle datasets download -d kmader/finding-lungs-in-ct-data
!unzip finding-lungs-in-ct-data.zip -d lung_segmentation

!unzip finding-lungs-in-ct-data.zip -d lung_segmentation

import os

for root, dirs, files in os.walk("lung_segmentation"):
    print(f"üìÅ {root}")
    for file in files[:5]:  # only list first 5 files to reduce output
        print("    ‚îî‚îÄ‚îÄ", file)

image_dir = "lung_segmentation/finding-lungs-in-ct-data/Lung Segmentation/images"
mask_dir = "lung_segmentation/finding-lungs-in-ct-data/Lung Segmentation/masks"

from glob import glob

image_paths = sorted(glob("lung_segmentation/**/*.tif", recursive=True))
print(image_paths[:5])

import os

for root, dirs, files in os.walk("lung_segmentation"):
    print(f"üìÅ {root}")
    for file in files[:5]:
        print("   ‚îî‚îÄ‚îÄ", file)

import cv2
import numpy as np
import os
from glob import glob

# This is the typical extracted structure
base_dir = "lung_segmentation/2D_lung_segmentation"

image_paths = sorted(glob(os.path.join(base_dir, "images", "*.tif")))
mask_paths = sorted(glob(os.path.join(base_dir, "masks", "*.tif")))

print(f"Found {len(image_paths)} images and {len(mask_paths)} masks")

images, masks = [], []

for img_path, mask_path in zip(image_paths, mask_paths):
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    if img is None or mask is None:
        continue  # Skip if either image or mask failed to load

    img = cv2.resize(img, (128, 128)) / 255.0
    mask = cv2.resize(mask, (128, 128)) / 255.0

    images.append(img)
    masks.append(mask)

# Convert to arrays
X = np.expand_dims(np.array(images), -1)
y = np.expand_dims(np.array(masks), -1)

print(f"Final dataset shape: X={X.shape}, y={y.shape}")

import os

for root, dirs, files in os.walk("lung_segmentation"):
    print(f"üìÅ {root}")
    for file in files[:5]:  # just show first 5 files per folder
        print("   ‚îî‚îÄ‚îÄ", file)

print("Number of images loaded:", len(X))
print("Number of masks loaded:", len(y))

import os

for root, dirs, files in os.walk("lung_segmentation"):
    print(f"üìÅ {root}")
    for file in files:
        print("   ‚îî‚îÄ‚îÄ", file)

base_dir = "/content/finding-lungs-in-ct-data/2D_lung_segmentation"

import os, cv2, numpy as np
from glob import glob

base_dir = "lung_segmentation/2D_lung_segmentation"
image_paths = sorted(glob(os.path.join(base_dir, "images", "*.tif")))
mask_paths = sorted(glob(os.path.join(base_dir, "masks", "*.tif")))

print("üñºÔ∏è Total images:", len(image_paths))
print("üé≠ Total masks:", len(mask_paths))

images, masks = [], []
for img_path, mask_path in zip(image_paths, mask_paths):
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    if img is None or mask is None:
        continue

    img = cv2.resize(img, (128, 128)) / 255.0
    mask = cv2.resize(mask, (128, 128)) / 255.0

    images.append(img)
    masks.append(mask)

X = np.expand_dims(np.array(images), -1)
y = np.expand_dims(np.array(masks), -1)

print("‚úÖ X shape:", X.shape)
print("‚úÖ y shape:", y.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def unet_model(input_size=(128,128,1)):
    inputs = Input(input_size)

    # Encoding
    c1 = Conv2D(16, 3, activation='relu', padding='same')(inputs)
    c1 = Conv2D(16, 3, activation='relu', padding='same')(c1)
    p1 = MaxPooling2D()(c1)

    c2 = Conv2D(32, 3, activation='relu', padding='same')(p1)
    c2 = Conv2D(32, 3, activation='relu', padding='same')(c2)
    p2 = MaxPooling2D()(c2)

    c3 = Conv2D(64, 3, activation='relu', padding='same')(p2)
    c3 = Conv2D(64, 3, activation='relu', padding='same')(c3)

    # Decoding
    u1 = UpSampling2D()(c3)
    u1 = concatenate([u1, c2])
    c4 = Conv2D(32, 3, activation='relu', padding='same')(u1)
    c4 = Conv2D(32, 3, activation='relu', padding='same')(c4)

    u2 = UpSampling2D()(c4)
    u2 = concatenate([u2, c1])
    c5 = Conv2D(16, 3, activation='relu', padding='same')(u2)
    c5 = Conv2D(16, 3, activation='relu', padding='same')(c5)

    outputs = Conv2D(1, 1, activation='sigmoid')(c5)

    return Model(inputs, outputs)

model = unet_model()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train, validation_split=0.1, epochs=10, batch_size=8)

# Predict on test images
preds = model.predict(X_test)

# Visualize predictions
n = 5
plt.figure(figsize=(12, 12))
for i in range(n):
    plt.subplot(n, 3, i*3 + 1)
    plt.title('Input')
    plt.imshow(X_test[i].squeeze(), cmap='gray')

    plt.subplot(n, 3, i*3 + 2)
    plt.title('Ground Truth')
    plt.imshow(y_test[i].squeeze(), cmap='gray')

    plt.subplot(n, 3, i*3 + 3)
    plt.title('Prediction')
    plt.imshow(preds[i].squeeze(), cmap='gray')
plt.tight_layout()
plt.show()



